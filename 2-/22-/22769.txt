Reichlich spät, aber immerhin. Doch dies beweist keineswegs, dass die USA 
keine Kolonialpolitik betrieben hätten und immer noch betreiben. Auch 
demokratische Rechtsstaaten können Kolonialpolitik betreiben. Dies 
beweisen die Vereinigten Staaten von Amerika. Dieses hübsche Bild aus dem 
Jahre 1898 zeigt, was Sache war.

Mit freundlichen Grüßen
Katharina