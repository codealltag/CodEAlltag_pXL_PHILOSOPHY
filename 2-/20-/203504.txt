[...]

Einverstanden.


Da bin ich nicht sicher, ob das nicht schon ueber die
Interpretation Ohlis hinausgeht. Das "Selbstmachen"
bezieht sich nicht nur auf bestimmte Zustaende _im_ System,
sondern auf das System selbst. Es konstruiert die Einheiten,
aus denen es besteht. Es steuert die eigene Reproduktion, 
die Aufrechterhaltung einer Grenze zur Umwelt autonom. 
Natuerlich stammt der Ansatz aus der Neurobiologie, 
vermutlich hat sich dort der Begriff der operationellen 
Geschlossenheit oder Selbstreferenz zuerst entwickelt und 
ist dann zu einer Autopoiesis-Theorie erweitert worden (?).


Ja, sicher. 


Schon klar, ich wollte nicht die theoretische Informatik unter 
Metaphysik-Verdacht stellen (ich selber habe ohnehin nicht die
grossen Probleem mit Metaphysik, da sich voellig ohne E. keine
Wissenschaftstheorie begruenden liesse - das hatten wir mal in 
einem anderen Thread).
Aber mir schien, du wolltest eben diese Aequivalenztheorien der 
Informatik zu einer allgemeinen Theorie (der Sprache, der Semantik,
des Verstehens?) erheben, wozu sonst sollte sie in diesem Kontext
bemueht werden. Und hier weiss ich nicht, ob man da sehr weit
kommt, d.h. gehaltvolle Aussagen ableiten kann. Man kann vielleicht
Aequivalenzen zwischen Rechnermodell und der Synatx einer 
bestimmten natuerlichen Sprache herstellen. Aber den Bezug zur Semantik,
zu den kognitiven Prozessen, die man in dieser Sprache so oder auch
anders ausdruecken kann - auch mit Hilfe syntaktisch unkorrekter
Redeweise, siehe deine Bemerkung oben - diesen Bezug klaert eine
solche Theorie nicht. Sie ist vielleicht nuetzlich fuer
Uebesetzungssoftware (englisch <-> abstraktes synatktisches Modell
<-> Chinesisch, oder auch: Grashalm <-> abstraktes synatktisches
Modell <-> Inhester-Sinfonie), aber inwiefern ist sie ein Argument
beim Chinesenzimmer-Problem?


Nun gut, es war salopp daher gesagt. Waehle eine angemessenere
Formulierung, und ich lasse trotzdem den Satz folgen:


Was ich eben sagte: Es muesste zunaechst geklaert werden, ob
wir den Menschen in der Weise als wohldefiniertes Rechnermodell
mit einer (einer? nehmen wir halt Chinesisch) syntaktisch 
wohldefinierten Sprache modellieren koennen, so dass man dann
Aequivalenzbeziehungen untersuchen kann. Angenommen, die CT-These
stimmt und wir haben solche Aequivalenzen. Dann sind wir aber
erst an dem Punkt, an dem das Chinesenzimmer-Beispiel _beginnt_:
Die voellig aequivalente Uebersetzung garantiert, dass der
Computer chinesisch "spricht" und, je nach Software, nicht nur
syntaktisch korrekte, sondern auch _fuer uns_ "sinnvolle" Saetze
sagt. Folgt also aus der CT-These, dass auch der Rechner 
"versteht", was er sagt?


Versteh mich nicht falsch. Wir koennen Massstaebe waehlen,
wie wir wollen: Wir koennen ihn so eng fassen, dass nur ich
intelligent bin und verstehe (im Gegensatz zum Rest der 
Menschheit), oder so weit, dass auch ein Teesieb intelligent
ist, weil es "weiss", dass es nur den Tee durchlassen soll
und nicht die Blaetter, um den Kalauer von A. Mosmann
anzubringen. Je weiter eine Definition ist, desto feiner
muss man eben Typ- oder Fallunterscheidungen treffen, wenn es
um konkrete Probleme geht, etwa die Unterscheidung der
Intelligenz des Typs, den biologisch basierte kognitive 
Systeme hervorbringen, vom Typ, den Rechnermodelle
hervorbringen, wobei diese Klassen wiederum zu unterteilen
sind... Kaaannn man machen, muss man aber nicht.

Mich verwundert es ueberhaupt nicht, dass sich Menschen
zur Begriffsbildung zunaechst an phaenomenologisch 
"offensichtlichen" Differenzen orientieren, und die Grenze
z.B. beim "Verstehen" beim Mensch ansiedeln. Vielleicht
aendert sich das spaeter einmal, aber dann wird es sehr
wahrscheinlich eine Differenzierung des Verstehensbegriffes
geben, wo die Unterschiede eben auf andere Weise ausgedrueckt
werden.

Gruss,

Edouard Hupach
mailto:n8qabg@zkaw.lqkb.lmb-svfl.ek
http://fuy.dqzo.wdi-miah.ao/Bycmb/pzxmun/stomh.sulh