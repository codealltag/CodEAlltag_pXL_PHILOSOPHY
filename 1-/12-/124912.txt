<snip technisches System am Beispiel Automobil>

Eine verlangsamte Entwicklung. Nicht das Machbare machen, sondern das
Machbare ersteinmal vollständig nachvollziehen und  sich dann nach dem
vollständigen Verständniss sich dafür oder dagegen entscheiden. Dazu
bräuchte es aber einen Konsens unter der gesamten Menschheit. Bekanntlich
wurde die Entwicklung der Atombombe forciert, nachdem die Angst bestand,
das Rüther, die Atombombe früher fertighaben könnte. Da wir uns schon
mitten im Informationskrieg befinden, ist es unwahrscheinlich, dass die
Entwicklung noch zu stoppen ist. Insbesondere nachdem die USA ihre
militärischen Überlegenheiten erhalten wollen. Genau das führt aber zu
einer forcierten Entwicklung bei jenen Staaten, die sich der USA
unterlegen fühlen. 


Was ist das aber dann, möglicherweise ist ja nichts anderes als eio
Fehler. Es gibt Tiere mit einem grösseren Gehirn und es gibt Tiere mit
einer grösseren Körpermasse. Der Mensch zeichnet sich nur durch eine
besondere Mischung aus. Angefangen vom Sprachorgan bishin zum Daumen an
der Hand, ist es das Zusammenspiel, welches hier wohl das Phänomen
Intelligenz und Bewusstsein hervorbringt. Die KI-Forschung berücksichtigt
dies in der Zwischenzeit und stattet Forschungssysteme in der Zwischenzeit
auch mit den körperlichen Attributen aus. Eines diese KI-Systeme hat sogar
gelernt zu schummeln, es sollte eigentlich fliegen lernen und benützte
dazu einen herumliegenden Bücherstapel und hat damit die Forscher genarrt. 


Ich bin mir sicher, das auch heutige Prozessoren Fehler enthalten, die
aber nicht nachgewiesen werden können, weil er im Rahmen der
Testverfahren, genau das tut, was er tun soll. Die "Fehlerfreiheit" wird
von Prozessorherstellern nur bei genauen Einhalten der Spezifikationen
garantiert. So kann es also durchaus sein, das ein und dasselbe Layout
eines Prozessors bei 1000 Mhz und bei 3000 Mhz fehlerfrei läuft.
Allerdings erfolgt nicht für jeden Chip die Freigabe für 3000 Mhz. Eine
"fehlerhafte" Produktionslinie läuft dann nur bei 1000 Mhz fehlerfrei. 
Statistische Methoden und Testverfahren liefern also "fehlerfrei" Produkte
aus, die aber als solches fehlerfrei gelten, weil kein Mensch einen Fehler
beweisen könnte, bzw. bis ein Mensch den Fehler nachgewiesen hätte, er
wahrscheinlich schon verstorben wäre. Das heisst, die Fehler würden genau
in den Bereichen auftreten, die wir eben nicht nachkontrollieren. Ein
kleiner Fehler kann aber eine grosse Wirkung haben. Die 5. Nachkommastelle
beim FDIV-Bug ist dabei schon ein Monsterfehler, weil er in kurzer Zeit
nachprüfbar ist. Nähmen wir an Pi, wäre an einer Terrabyte stelle irgendwo
plötzlich endlich, weil sich ein Computer aber verechnet hat, würde dieser
weiterrechnen, weil für den Computer eben nicht endlich. Bis wir manuell
das Ergebniss überprüft hätten, wären wahrscheinlich Generationen
vergangen.  


Theo Baumgartl hat mal ein Projekt vorgeschlagen, das die Erzeugung eines
künstlichen Bewusstseins auf der Ebene eines weltweit verteilten
Rechnernetzwerkes möglich lassen würde. Der Vorschlag stammte aus den 60er
Jahren und er hat ihn in den 90er Jahren wiederholt. Ob ein solches
Konzept funktionieren würde oder nicht, ist schwer zu sagen und der Nutzen
ist ja auch äusserst gering. 


Wenn wir Bewusstsein als materiell messbar vorraussetzen, dann müsste bei
einer exakten strukturellen Kopie, samt allem was wir messen können, genau
dasselbe Ergebnis herauskommen. Da wir aber nicht mal fähig sind ein
einziges Lebewesen lebendig zu kopieren noch nicht einmal eine Mikrobe,
ist der Beweis, das es unterschiedliche Bewusstsein auch bei einer exakten
Kopie gibt, schwer anzutreten. Möglicherweise sind Lebendkopien ja genau
aus dem Grund nicht möglich. Vielleicht stehen ja auch physikalische
Gesetze dem entgegen, das verschiedene Materie ein gleiches Bewusstsein
haben könnte.

Grösse,
Nils