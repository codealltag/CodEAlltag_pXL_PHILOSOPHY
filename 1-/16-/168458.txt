In article <10BZR44N.61NF6Y6E@ujxbepq.so>,


Akzeptiert. Ich sagte ja, emotionale Standpunkte können den Blick für
das wesentliche Trüben... ist mit auch schon oft genug passiert :-)


Das Problem liegt wohl unter anderem in der allgemeingebräuchlichen
Definition des Begriffs "Verstehen", der sich scheinbar dann doch von
der philosophischen unterscheidet. MMn "versteht" im allgemeinen
Sprachgebrauch eine Entität, wenn sie den Anschein hat, zu verstehen.
Man vergleicht ihr Verhalten mit dem von Entitäten, von denen wir
hinreichend sicher sind, daß sie "verstehen" (gut bekannte Menschen).
Wenn genügend Ähnlichkeit besteht, sagen wir, die Entität "versteht".
 Daher sprechen z.B. Tierbesitzer ihren Haustieren oft die Fähigkeit zu
Verständis (wenn auch nur in einem begrenzten Bereich) zu. Und daher
rührt auch der Auguste Effekt (einem "simplen" Program
Verständnisfähigkeit zu zuschreiben) - dem immer wieder auch solche
Menschen anheim fallen, die es eigentlich besser wissen müßten.

Das bedeutet, daß "Verstehen" immer von dem abhängt, der darüber eine
Aussage macht. Entweder das "Selbst", daß über die eigenen
Gedankengänge reflektiert (hat das nicht Kimmlingen so ähnlich
formuliert?), oder aber ein externer anderer. Wenn ich das Gefühl habe,
eine andere Entität versteht (mich)... darf ich ihr dann die Fähigkeit
zu Verstehen absprechen, nur weil sie kein Mensch ist? Keine einfache
Frage... aus ethischen Überlegungen heraus würde _ich_ im Zweifel für
die potentielle Verst�ndisf�higkeit der Entität sprechen.

Manchmal befürchte ich fast, daß einige der sehr starrköpfigen KI-
Kritiker ihre Meinung aus dem Grund so lautstark verkünden, weil sie
insgeheim Angst haben, welche indirekten (aber eventuell
folgenschweren) Auswirkungen die akzeptierte Existenz einer "bewußten"
KI auf unser Weltbild, insbesondere auf unsere Moralvorstellung haben
könnte. Vermutlich haben würde.

Aber zu deiner eigentlichen Frage: "Kann es Maschinen geben, die
anscheinlich verstehen, aber kein Bewußtsein besitzen" (Richtig?)
MMn ja. Ich sage nur: Der Eliza Effekt. Aber, und darauf kommt es mir
an: Jede Maschine, die anscheinlich versteht, aber kein Bewußtsein
besitzt, wird sich (früher oder später!) als solche entlarven lassen.

Ich bin tatsächlich der Auffassung (keine große Überraschun, hmm?), daß
es warscheinlich möglich sein müßte (hinreichend leistungsfähige
Hardware und hinreichend Zeit, die Software zu entwickeln bzw. sich
selbst entwickeln zu lassen vorrausgesetzt natürlich), einem
Digitalcomputer Bewußtsein "beizubringen". Ich bin mir nicht 100%
sicher - es ist nur eine starke Vermutung, gebildet aus meinem Wissen
über die Welt.

Dieses Programm würde sich mit Sicherheit in einigen wesentlichen
Dingen von dem unterschieden, was momentan als "KI" verkauft wird.

Aber, das ist natürlich kein Grund, warum es so etwas pinzipiell nicht
geben könnte. Zwei Gründe, die mich dazu bringen, zu vermuten, daß es
prinzipiell möglich sein sollte:

(1) Der Mensch kann als biologische Maschine beschrieben werden, die
sich in einem kontinuierlichem Prozess aus nicht-verstehenden
biologischen Maschinen entwickelt hat. Natürlich spielen für
das "Verstehen" Umwelt, Sprache, Kultur etc. eine Rolle, aber das ist
letztendlich nichts, was bei einer Maschine nicht auch dieselbe Rolle
spielen könnte! (bzw. funktional ersetzt werden könnte). Ich gehe in
dieser Beziehung mit den Ansichten von Jörn Buckstöver konform.

(2) Ich habe "Consciousness Explained" von Benedikt P Brucklacher gelesen,
und die Kernaussage seines Modells für Bewußtsein _verstanden_. Bilde
ich mir wenigstens ein. Es stimmt in erstaunlichem Maße mit dem
überein, was ich durch Introspektion über meine persönlichen
Denkvorgänge erfahren habe. Dennetts These ist (aufs gröbste
verstümmelt), daß Bewußtsein letztendlich nur "eingebildet" ist, ein
emergentes Phänomen der Zusammenarbeit vieler unbewußter Teilsysteme
darstellt. Hmm... ich fürchte, dieser Satz ist warscheinlich sehr
leicht mißzuverstehen.
 Jedenfalls habe jetzt ich eine intuitive Vorstellung davon entwickelt,
wie das "selbst erfahrene Bewußtsein" ein "Effekt" einer bestimmten Art
und Weise von geordneter Informationsverarbeitung sein könnte. Ohne
jeden metaphysischen Pipapo, und ohne Schiffmachers Quantenphänomene (die
ich nicht ausschliessen will, aber für rein spekulativ halte).
 Als Folge vermute ich ganz stark, daß eine Entität, die sich _genau_
so wie ein Mensch verhält (d.h. den härtesten Tests standhält, die man
sich vorstellen kann), auch ein ähnliches "Selbstgefühl" im Kopf haben
müßte (da will ich aber nicht wirklich ausführlich argumentieren,
Brucklacher kann das viel besser).

Was ist notwendig für "Bewußtsein" in einer qualitativ ähnlichen Art
(ein sicherlich dehnbarer Begriff) zu dem, was wir in uns selbst
erfahren?
 Ich vermute unter anderem ein (mehr oder weniger) detailliertes Modell
der eigenen Denkvorgänge zu besitzen, welches parallel zu den
eigentlichen Denkvorgängen "abläuft", und diese "beobachtet"
(Parallelen identifiziert, Vergleiche anstellt). Vielleicht auch viele
geschachtelte Stufen dieses gegenseitigen Beobachtens. Das aber ist
kaum mit den uns heute zur Verfügung stehenden Mitteln zu erreichen,
jedenfalls nicht mit einer Maschine, die dann auch noch Zeit und Platz
dafür hat, etwas "sinnvolles" zu leisten :-)
 Es könnte da durchaus eine gewisse minimale Komplexitätsschwelle
geben, unter der so etwas nicht modellierbar ist.

[...]

Da stimme ich dir zu, aber das habe ich ja schon im ersten Posting
getan.

Ich habe eben über Maschinen mit "Bewußtsein" geredet. Man darf aber
nicht vergessen, daß dies im Grunde gar nicht das eigentliche Ziel der
KI ist (!) Die Zielstellung, so wie ich sie verstehe, ist, die von
außen beobachtbaren kognitiven Fähigkeiten von Menschen ganz oder
teilweise nachzubilden, mit den Ziel, irgendwann einmal etwas noch
Leistungsfähigeres zu entwickeln. Wie du siehst geht es eigentlich gar
nicht darum, ob die Maschine nun Bewußtsein hat oder nicht, sondern ob
ihre Leistung für uns von Nutzem sein wird. Dem Großteil der KI ist es
wohl egal, was die Maschine "denkt" (mir nicht :-), solange das was sie
ausspuckt für _uns_ Bedeutung hat.


Hmm. Du hast keine große Erfahrung mit Software-Entwicklung (bei
komplexen Problemstellungen), oder? Ich sage das nur, weil es meiner
Erfahrung nach für Laien extrem schwierig ist, ein intuitives Gefühl
davon zu bekommen, was dort möglich ist und was nicht.


Aber genau darum geht es der modernen KI im Grunde hauptsächlich.


Da möchte ich ganz leicht einhaken: Für _den Computer_ bedeuten die
Ergebnisse vielleicht nichts (für die heutigen wenigstens), aber für
_uns_ haben sie in der Regel schon Bedeutung!
 Wenn du so willst kannst du die Rechner damit als "Erweiterung"
unserer kognitiven Fähigkeiten betrachten, als "externe Auslagerung"
kognitiver Prozesse (ähnlich den Metaphern von Ulber). Was dann zu
ganz anderen interessanten Diskussionsthemen führt :-)


Was u.A. auch daran liegen mag, daß wir in Wirklichkeit die viel
schnelleren Rechner sind. Die gesammte Rechenleistung aller Computer
auf der Welt ist immer noch gering relativ zur
theoretischen "Rechenleistung" _eines_ menschlichen Gehirns. Wenn die
gegenwärtigen technologischen Trends anhalten, könnte sich das aber in
30 Jahren oder so ändern. Kurzum: Ich glaube kaum, daß man die "real
gezeigte Leistung" eines menschlichen Gehirns auf Hardware simulieren
kann, welche effektiv deutlich weniger Leistungsfähig ist als das
Gehirn.

Udo Valcke


Sent via OVR http://dtf.iifg.xmk/
Before you buy.