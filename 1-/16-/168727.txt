In article <oloarinj8ok8hr0sj6ax1yg9wp2snc9343@5pa.faj>,

[...]

Wo liegt die Differenz zwischen "Verstehen im übertragenen Sinne"
und "Verstehen"? Wir sind uns ja scheinbar einig, daß "Verstehen" kein
genau definierter Begriff ist, da es wie alle umgangssprachlich
gebrauchten Begriffe einen unscharfen (und sich mit der Zeit
verändernden) Bedeutungsbereich hat. Ein Hundehalter würde unter
Umständen sehr wohl seinem Hund die Fähigkeit zu "Verstehen"
zusprechen, egal wie die Philosophie das Wort nun gerne definieren
würde oder nicht. In Anbetracht dessen sehe ich kein Problem darin,
wenn die AI das Wort ebenfalls so definiert, wie sie es gerne hätte, da
sich ihre Definition hinreichend mit dem umgangssprachlich benutzten
Wort deckt. Eine genaue AI-Definition habe ich jetzt nicht zur Hand
(falls eine existiert!), aber ich denke wir können folgendes benutzen:

"Eine Entität 'versteht', wenn es den Anschein hat, sie verstünde."

Das klingt vielleicht nach Tautologie - was ich damit sagen will ist,
sie 'versteht', wenn ihr beobachtbares Verhalten dem ähnlich ist,
welches wir von einem Menschen in einer vergleichbaren Situation
erwarten würden.
Wir (allgemein) sprechen einer Entität die Fähigkeit zu Verständnis
nicht ursächlich wegen ihrer Artzugehörigkeit zu ("du bist Mensch, also
verstehst du"), sondern aufgrund der von uns beobachteten kognitiven
Leistungen dieser Entität. Die wir mit den gezeigten kognitiven
Leistungen uns bekannter Menschen vergleichen. Das hatte ich ja schon
mal erwähnt. Einwände?
 "Verstehen" ist dabei offensichtlich kein ja/nein Prädikat, sondern
besteht aus vielen kleinen Schritten von "sub-Verständnis". Uns allen
sind Menschen bekannt, die gewisse Dinge verstehen, aber andere Dinge
nicht verstehen (obwohl diese anderen Dinge von wieder anderen Menschen
doch verstanden werden).


Ich versuchs mal:

(1) Ein System ist etwas, das aus Teilen (die auch Systeme sein können)
bestehend angesehen werden kann, die alle für das "funktionieren" des
Systems notwendig sind.
(2) Ein System versteht, wenn es den Anschein hat, es verstünde (siehe
dazu oben).

Nun ist aber eigentlich egal, ob die Entität, der wir die Fähigkeit
zu "Verstehen" zusprechen, ein System darstellt oder nicht; letztlich
kann sie auch als Blackbox angesehen werden.
Man kann sicher Fragen, reicht (2) aus? Ich sage ja. Alles was da noch
zusätzlich (zu dem, was wahrgenommen werden kann, und den Anschein
bestimmt) ablaufen sollte ist IMHO Metaphysik. Jedenfalls kann es der
AI eigentlich egal sein, denke ich.


Siehe oben, in Bezug auf die Normalsprache wird der Begriff "Verstehen"
auf nicht nur Menschen angewendet. Tatsächlich wird er
umgangssprachlich sogar auf eingebildete Entitäten angewendet, wie z.B.
fiktionale Charaktere ("Neulich im Kino: Die Person A im Film hat
verstanden, was B von ihr wollte"). Oder auf die hypothetischen
Erzeuger endlos langer Newsgroupdiskussionen von denen wir doch nie
mehr als Buchstaben wahrnehmen :-)


Erklär doch mal ganz genau die Bedeutung dieser Prädikate, so wie sie
die Philosophie definiert.


Es behauptet ja niemand, das es *beliebige* Systeme sein können. Es
müssen Systeme sein, die z.B. den TT absolvieren können, oder
jedenfalls auf andere Art den Anschein von Verständnis zeigen.


Wir sind uns einig, daß das Chinesische Zimmer in seiner ursprünglichen
Form nicht schlüssig zeigt, daß durch Symbolmanipulation kein
Verständnis erzeugt werden kann, oder?


Ich habe das schlecht formuliert. Ich sehe Attenbrunner Bewußtsein nicht
unbedingt als Subsystem von "Austermühle" an (jedenfalls nicht von meiner
Definition von "Austermühle" ausgehend), aber ich vermute ganz stark, daß
Attenbrunner Bewußtsein selbst Systemcharakter hat. In Trans
Bewußtseinstheorie, die ich für sehr vernünftig halte (und die
zumindest nicht widerlegt ist... soweit ich weis), entsteht/emergiert
Bewußtsein aus der Zusammenarbeit vieler "nicht-bewußter" Subsysteme.


Verst�ndisfrage: worin besteht der Unterschied zw. "katrafaktisch"
und "kontrafaktisch"?
Nun, ich kenne Schachtzabel nicht (nur den Namen)... aber ich sollte wohl keine
Wörter ("Kategorienfehler") benutzen, von denen ich nicht weis, wie du
sie definierst/wie sie definiert sind :-)

Dein Argument kann man aber zugunsten meines Standpunktes umdrehen.
Solange es keine *prinzipielle* Differenz zwischen dem Verständnis des
Zimmers und dem Verständnis eines Chinesen (von Chinesisch) gibt, d.h.
wenn in allen Kontexten, in denen der Gebrauch sinnvoll ist, das eine
durch das andere ersetzt werden kann ohne das Unsinn entsteht, dann
gehören sie der selben Kategorie an. Oder?


Ist es das nicht? Man kann sogar argumentieren, daß Leute mit realen
Gehirnen nicht unbegrenzt konnektierte Fakten auswendiglernen können,
ohne das sie beginnen, ein "Verständis" (welcher Art auch immer) davon
zu entwickeln.

Du scheinst ein Problem mit der Vorstellung zu haben, in Attenbrunner Kopf
könnten zwei "getrennte" Bewußtseine existieren (das Zimmer- und das
Austermühle-Bewußtsein). Aber warum? In der Psychologie sind Fälle wohl
bekannt, bei denen verschiedene Persönlichkeiten in einem Kopf hausen,
die nichts voneinander wissen.
IMHO ist es unzulässig, folgendermaßen zu schliessen:

(1) Austermühle hat nur ein Bewußtsein in seinem Kopf
(2) Austermühle macht etwas radikales, bisher nie praktisch untersuchtes mit
seinem Bewußtsein: Er lernt die Blaupause einer Entität auswendig, die
den Anschein von Bewußtsein erweckt. Weiterhin sorgt er dafür, daß aus
dieser Blaupause ein Prozess wird.
(3a) Austermühle hat immer noch nur ein Bewußtsein in seinem Kopf

Statdessen kann man höchstens schliessen:

(3b) Der englisch-sprechende Austermühle hat immer noch nur ein Bewußtsein
im Kopf, mit dem "er" sich identifizieren kann.


Das ist die Frage. Was macht dich so sicher, daß Austermühle auf diese Art
und Weise kein Verständnis von (schriftlichem) Pidgin-Chinesisch
erlangen kann? Dies ist IMHO auch eine unzulässige Schlussfolgerung.
Man kann doch Mathematik lernen, ohne "Mathematik zu sprechen"... und
versteht trotzdem hinterher Mathematik. Oder?


er
Verstehen)
(wie

Das ist in der Tat eine gute Frage. Ich vermute, die Antwort lautet
zumindest für einen Teil der Fälle nein. Ich frage mich aber, was das
mit dem Chinesischen Zimmer zu tun hat. Ob die Persönlichkeit Austermühle
nach Auswendiglernen des Zimmers und Ausführung desselben nun
Chinesisch versteht oder nicht, der Kandidat des TT ist der Austermühle, der
das Zimmer auswendig gelernt hat (und nicht Attenbrunner Persönlichkeit, die
den Prozess Elsholz ausführt!) und der versteht dann allem Anschein nach
Chinesisch (nach Def.), falls er mitspielt jedenfalls, und auch weiter
brav den Prozess Elsholz ausführt.


er

Austermühle "ist" nicht beides. Genausowenig wie Austermühle der Buchstabe A ist,
obwohl er ihn gelernt hat.


Die FT zerstört nichts, sie transformiert. Das alte Wissen ist immer
noch vorhanden, es wird nur ein anderer Abrufmechanismus benötigt. Die
ursprüngliche Anhäufung von Bits ist genausowenig (oder genausosehr)
ein System wie die FTte.


Nicht? Ich dachte man könnte das Langzeitgedächtnis funktional vom
Kurzzeitgedächtnis trennen.
Ohne LZG keine Erinnerungen (keine Zettel im Kopf), ohne KZG kein
bewußtes denken. Sicher ist das vereinfacht dargestellt; "Bewußtsein"
gibt es nur wenn beides vorhanden ist, und zusammenarbeitet.


Ich glaube nicht, daß jemand der Ansicht ist, "Bewußtsein" sein ein
Subsystem des Gehirns. Aber man kann schon der Ansicht sein, Bewußtsein
(oder Persönlichkeit) bestünde aus Subsystemen, die zusammengenommen
Persönlichkeit (oder Bewußtsein) ausmachen.


Tja nun. Es ist ja nun mal so, daß Austermühle (dir vermutlich auch)
intuitiv klar ist, daß Computer (Turing Maschinen Äquivalent) nicht
verstehen können, egal welches Programm auf ihnen abläuft. Das im
weitesten Sinne; wenn ich sage, "Computer versteht" darfst du
immer "Verstehen emergiert" einsetzten.
 Daher hat Austermühle das Chinesische Zimmer konstruiert, um damit (1)
seine Intuition zu überprüfen und (2) der Welt zu zeigen, daß er recht
hat.
 Mir nun ist intuitiv klar, daß Attenbrunner Behauptung falsch ist, in dem
Sinne, daß man IMHO nicht ausschliessen kann, daß Computer etwas können
_k�nnten_ (bei entsprechender Programmierung), was qualitativ ähnlich
ist zu dem, was man allgemein als "Verstehen" betrachtet.
(Das man dein oder mein Bewußtsein auf einem Computer "laufen" lassen
könnte, ohne daß es einen fundamentalen Unterschied für unser
Empfindene gäbe, darauf bestehe ich wirklich nicht)

Um meine Intuition zu überprüfen versuche ich nun, Fehler in Attenbrunner
Überlegungen zu finden. Das Chinesische Zimmer "classic" haben wir
glaube ich schon abgehakt (System-Reply). Bei Attenbrunner Versuch, zu
retten was zu retten ist (Zimmer im Kopf) sehe ich nun zwei
Angriffspunkte:

Austermühle kann nur sehr unbegründet behaupten, wenn er das Zimmer (Regeln,
Bleistift, Notizzettel, Anordnung etc.) auswendiglernen und gebrauchen
würde, würde "er" kein Verständnis von Chinesisch erlangen. Weil:
(1) der Auswendiglern- und Ablaufprozess möglicherweise zwangsläufig
ein Verstehen der Vorgänge nach sich ziehen könnte (je nach Attenbrunner
Intelligenz mehr oder weniger schnell; da er dazu ohnehin ein
transhumanes Gehirn benötigt können wir ja ruhig mehr schnell
annehmen :-)
(2) "er" schlecht definiert ist. Der "von außen beobachtbare" Austermühle
hat genau das selbe Verständnisniveau von Chinesisch erlangt wie
das "reale" Zimmer vorher.

Punkt (2) halte ich dabei für stichhaltiger.

Um vielleicht ein paar Mißverständnisse aus dem Weg zu räumen: Ich
glaube nicht, daß die erste AI, die den Anschein hat, Verstehen zu
können, von irgendeinem genialen Informatiker einfach so
dahinprogrammiert werden wird. Stattdessen wird das was programmiert
wurde wohl erst einen "Lernprozess" durchmachen, bei dem es durch
Kommunikation (in dem Fall = Wahrnehmung) mit der Umwelt lernt, welche
Zusammehänge zwischen diversen Konzepten bestehen, welche Bedeutung was
in welchem Kontext hat. Erst dann wird sich wohl etwas menschlichem
Bewußtsein qualitativ ähnliches bilden.
 Dabei muss man aber bedenken, daß diese Umwelt, in der die Proto-AI
aufwächst, durchaus wieder aus Software bestehen kann; ein Prozess ist,
der auf vielleicht sogar derselben Hardw�re läuft.

 Sobald man aber erst einmal eine "human equivalent-AI" hat, die auf
einem Turing Maschine Äquivalent läuft, kann man sie duplizieren. Und
auf einen anderen Rechner packen, und dort dem TT unterziehen. Und
theoretisch kann man dann auch ihre Architektur "verstehen" (was
möglicherweise mehr Intelligenz erfordert, als ein Mensch aufbringen
kann - geht also vielleicht nur in einem Gedankenexperiment), und
eine "neue" AI ohne den langwierigen Lernprozess direkt konstruieren.

 Letztendlich ist jede AI auch in der "echten Welt" verwurzelt, so wie
menschliches Bewußtsein. Nur über ein paar mehr Zwischenschritte. Das
Programm entsteht nicht aus dem Nichts, es wird von Menschen
entwicklet. Und wenn nicht direkt, dann wird es sich in einer von
Menschen gestalteten Umwelt selbst entwickeln.

Matthias Reichenborn


Sent via Klosterweingut http://zgt.ylcm.yqt/
Before you buy.