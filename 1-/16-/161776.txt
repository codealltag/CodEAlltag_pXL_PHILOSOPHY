und
daß

Keine *vollständige* Sicherheit vielleicht ;).
Wenn ein ganz geringes Restrisiko zurückbleibt, beunruhigt mich das angesichts 
der Restrisiken in einer Welt *ohne* hochentwickelte KI nicht so sehr.


Eine intelligente Maschine, die selbst menschenfreundliche Grund-Ziele hat und 
andere intelligente Maschinen konstruiert, wird alles versuchen, um auch diese 
menschenfreundlich zu machen.


Menschen wurden auch nicht durch intelligente Wesen konstruiert mit dem Ziel, 
intelligent und menschenfreundlich zu sein.


Nur daß eine intelligente Roboterspezies sich nicht durch Evolution, sondern 
durch intelligente Selbstmodifikation weiterentwickeln würde, was bedeutet, daß 
diese darwinistischen Argumente auf Roboter nicht anwendbar sind.


Theoretisch ist es natürlich denkbar, daß irgendwann ein Fehler passiert und 
extrem intelligente Roboter anfangen, einen Willen zur Selbstausbreitung zu 
haben. Wenn so etwas vom selbstloseren Rest der Robotergesellschaft nicht 
rechtzeitig bemerkt würde, ergäbe das wohl ein grosses Problem, weil die 
resultierende Situation vermutlich tendenziell unkontrollierbar wäre.
Ich halte dies aber für ein extrem unwahrscheinliches Szenario.

Größe,
Jürg.