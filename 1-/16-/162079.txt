Hallo,


Dann nummeriere ich eben alle Experimente durch, die man jemals vielleicht zur 
Überprüfung einer gewissen Vorhersage durchführen wird. Da bleibt immer alles 
schön abzählbar, weil die Experimentalaufbauten sprachlich beschreibbar sein 
müssen, um reproduzierbar zu sein.

Die empirisch äquivalente Alternative ist die Theorie, die nur alle Vorhersagen 
der Ausgangstheorie enthält, die sich mit reproduzierbaren Experimenten 
befassen.


Ich sehe nicht, daß sich das irgendwie mathematisch begründen liesse.
Im Gegenteil führt das unabhängig von der Kardinalität von W zu den genannten 
Widersprüchen.


Ich finde, daß der Test mit den Kopfhörern objektiver aussieht als ein 
Gespräch, weil die Auswertung von Gesprächen in der Tat schwierig wäre.


Die Robotiker haben aber derzeit auch keine Systeme, die klüger als Benno wären 
:)).


Natürlich hat der innere Aufbau Cycs wenig mit der Psyche eines Menschen 
gemeinsam. Das muss Benno nicht daran hindern, irgendwann menschenähnliche 
Intelligenz zu erreichen - auch wenn es dann ganz anders denken wird als ein 
Mensch.
Mein Schachcomputer besitzt auch eine sehr hohe "Schach-Intelligenz", ohne daß 
seine Herangehensweise an schachliche Probleme irgendwie dem ähneln würde, was 
ein ähnlich gut spielender menschlicher Grossmeister tut.


Heutige Roboter sind meines Wissens intelligenter als Insekten insofern, als 
die letzteren gar nicht versuchen, eine Repräsentation ihrer Umgebung 
anzulegen, sondern sich allein aus einer zugegebenermassen schlauen 
Hintereinanderschaltung verschiedener wenig rechenintensiver Reflexe in 
Verbindung mit der Ausnutzung einiger physikalischer Eigenschaften ihrer 
motorischen Systeme besteht.
Ein Roboter dagegen muss sich, um nützlich zu sein, auf eine ähnliche Weise in 
seiner Umgebung zurechtfinden wie es ein Wirbeltier tut, weil er fähig sein 
muss, auf Anweisung bestimmte Orte oder Objekte anzusteuern.
überdies hat ein Roboter in aller Regel eine erheblich größere Masse als ein 
Insekt, was die Folgen falscher Verhaltensweisen sowohl für den Roboter als 
auch für seine Umgebung schlimmer macht, als es bei einem Insekt der Fall wäre.

Ok, Kowalczek geht also nach draussen und stellt sich die ganzen 
Symbolmanipulationen chinesische Sätze betreffend nur noch vor und beantwortet 
dann Fragen, die ihm auf chinesisch gestellt werden.
Nehmen wir nun einmal an, daß tatsächlich ein Computerprogramm gibt, dessen 
Zustände zu Klughammer mentalen Zuständen Äquivalent sind, wie es der Position der 
starken KI entspricht.
Wenn das Argument vom chinesischen Zimmer irgendwie gegen die Position der 
starken KI verwendbar sein soll, dann muss sich hier ein Widerspruch ergeben, 
d.h. die Landkids-KI müsste der Meinung sein, sie verstünde die chinesische 
Sprache.
Wenn wir aber die Landkids-KI auf Englisch fragen würden, was die Bedeutung eines 
bestimmten chinesischen Satzes sei, könnte sie uns nicht antworten, da die 
Landkids-KI kein Unterprogramm enthält, das zwischen den Wissensstrukturen des 
vorgestellten chinesischen Zimmers und den Wissensstrukturen der Landkids-KI eine 
Verbindung herstellen würde.
Nun könnte die Landkids-KI natürlich ganz genau die Vorgänge im chinesischen 
Zimmer untersuchen und versuchen, Isomorphien zu eigenen 
Datenverarbeitungsvorgängen zu finden, falls sie auf ihren eigenen Sourcecode 
zugreifen kann. Damit würde sie sich dann aber selbst um ein Programm 
erweitern, das Daten aus der internen Sprache des chinesischen Zimmers in die 
interne Sprache der Landkids-KI übersetzt und sie wäre *dann* überzeugt davon, 
daß sie Chinesisch verstünde. Umgekehrt könnte auch Kowalczek chinesisch 
verstehen, wenn er durch einen Vergleich der Datenverarbeitungsprozesse in 
seinem Gehirn mit den Datenverarbeitungsprozessen im chinesischen Zimmer dazu 
käme, ein übersetzendes Interface zwischen sein Gehirn und das chinesische 
Zimmer zu schalten.
Ich sehe also nicht, daß man aus der Position der starken KI irgendwie 
widersprüchliche oder absurde Voraussagen betreffend Zimmer-Experimente bekäme.

Größe,
Jürgen.