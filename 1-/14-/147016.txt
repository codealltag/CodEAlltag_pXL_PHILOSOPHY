Die KI muss etwas optimieren (z.B. wenn wir dem Utilitarismus folgen
die "Summe des Glücks", wie auch immer definiert).  Das Optimum zu
erreichen können wir als "Wunsch" oder "Ziel" der KI betrachten.  Eine
Intention ist also da.

Die KI muss danach Strategien suchen, um aus der augenblicklichen Lage
heraus die Zielfunktion zu optimieren.  Dazu muss sie alle Tricks und
Kniffe der Spieltheorie beherrschen.  Die Reaktionen der Menschen kann
sie nur statistisch voraussagen (es sei denn Menschen würden zu
Puppen).  "Fürchten" und "Hassen" entstehen dabei vermutlich
automatisch.  "Furcht" wäre ein Zustand der KI, in dem sie eine
Variante der Reaktion auf eine Handlung, die ihrem Ziel entgegensteht,
zwar für nicht allzu wahrscheinlich hält, aber extrem negativ wertet.
Sie summiert dann die Wahrscheinlichkeiten auf, und kommt zu dem
Schluss, dass die Handlung sich wegen dieser Gefahr nicht lohnt.


Das ist die Frage der Auswahl der Zielfunktion. 


Das sehe ich weniger als Problem.  Die KI hat ihre Zielfunktion, und
entscheiden welcher Ausgang des Konflikts ihre eigene Zielfunktion
besser optimiert sollte sie können. 


Ich würde eher sagen jeder hat seine eigene, und man kann sich nicht
drüber einigen.  Die KI hat halt ihre Metrik dazu - an sich kein
Problem, jedenfalls kein anderes als das Auswahlproblem bei der
Gl�cksfunktion.


Die KI legt natürlich ihre eigene physikalische Theorie von der Welt
(samt damit verbundener Beschreibungsweise) ihren Entscheidungen
zugrunde.  Auch hier natürlich wieder das altbekannte Auswahlproblem,
aber eher geringer. Schliesslich kann man sich auf ART und SM einigen,
und die sinnvollen Varianten von ART und SM haben eher geringen
Einfluss auf wichtige politische Entscheidungen.


Das Problem des naturalistischen Fehlschlusses?  Nun, das wäre durch
die Auswahl der Zielfunktion ja gelöst.  Insofern auch kein neues
Problem.


Nun, für die praktische Anwendung reicht es ja, wenn sie besser ist
als der Mensch, der ja selbst oft genug irrt.  Wir verwenden ja
genügend fehlbare Maschinen.  Manche Leute sollen ja sogar JKJR
verwenden, SCNR.


Nun, nach Fickenschers Argumentation können wir das sowieso nicht, weil wir
ja sowieso selbst entscheiden müssen, ob die Empfehlungen der KI gut
oder böse sind.  Was im wesentlichen darauf hinausläuft, ob die Ziele
der KI gut oder böse sind.

Und obwohl die KI intelligenter ist als ich, kann ich auch die Gründe
nachprüfen.  Schliesslich ist es erheblich schwerer, eine optimale
Strategie zu finden (macht die KI), als sie, wenn sie gefunden ist, zu
begründen (sollte sie also auch hinkriegen), oder gar nur Teile der
Begründung zu verstehen (was für mich machbar ist).

Silvio
-- 
F. Zurn,  <pjae@ehbt-qspeemylj.xzi>, http://hhbt-pvpyqyttp.hvr