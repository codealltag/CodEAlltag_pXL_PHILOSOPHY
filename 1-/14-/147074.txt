Hallo.

Seit einigen Tagen verfolge ich (wenn auch nur oberflächlich) die Diskussion
über die Möglichkeit eines künstlichen Regierungsroboters (i.F. "KI"), die
aus xo.gmc.vsxvmbugxp.to hier herüber geschwappt ist. Wenn sich meine u.s.
Kritik nur auf einzelne der geäusserten Meinungen beschränken würde, hätte
ich natürlich keinen neuen Thread anzuzetteln brauchen. Ich bin jedoch der
Auffassung, dass diese Meinungen aus einigen Grundannahmen resultieren, die
ich für verfehlt halte.

Zunächst stellt sich die Frage, ob oder wie die Annahme gerechtfertigt
werden kann, dass intelligentes Verhalten vom Kaliber der o.g. KI ohne
intentionale Einstellungen wie Wünschen, Wollen, Fürchten, Hassen, etc.
überhaupt vorkommen kann. Immerhin soll sie sich ja neutral verhalten,
andernfalls könnte man ihr kaum vertrauen. Selbst, wenn man annimmt, dass
die KI nur das Gute will und das Böse ablehnt, stellt sich im Anschluss die
Frage, ob es überhaupt etwas objektiv Gutes oder Böses gibt. Ich glaube das
zwar, aber das tun auch viele andere, die in Bezug auf die Bewertung
moralisch relevanter Sachverhalte entgengesetzter Ansicht sind als ich. Man
kann zumindest nicht annehmen, dass die KI ausgerechnet die eigenen
moralischen Lieblingsideen als das objektiv Gute identifizieren wird.

Aber selbst wenn das irgendwie gelöst wäre, würden damit noch keine
moralischen Konflikte ausgeschlossen. Man könnte sich zwar auf den
Standpunkt stellen, dass in einem moralischen Konflikt immer eine der zwei
entgegengesetzten Positionen recht hat oder zumindest stärker ist als die
andere und die KI das wegen ihren überragenden Fähigkeiten erkennen kann,
aber das scheint mir kaum begründbar zu sein. Wer so eine KI möchte, müsste
zuerst eine Metrik zur Auflösung moralischer Konflikte vorlegen. Wie die
menschliche Gesellschaft zeigt, ist bisher keine solche Metrik bekannt.

Da es inkommensurable Beschreibungsweisen in Bezug auf die Welt geben kann,
könnte es auch sein, dass die im Konflikt stärkere Position je nach
Beschreibungsweise eine andere ist. Dies ist nicht bloß ein
epistemologisches, sondern vielmehr ein ontologisches Problem, und es ist
grundsätzlich nicht lösbar. Man könnte sich zwar für jeweils eine der
konkurrierenden Beschreibungsweisen entscheiden, aber nur auf Grund anderer
Kriterien als moralischer, sonst käme man in einen Erklärungszirkel. Wenn
das aber funktionierte, hätte man damit Moralität auf etwas anderes
reduziert, und auch das schaut nicht so aus, als wäre es überhaupt möglich,
wie die vielen gescheiterten Versuche hierzu zeigen.

Abgesehen davon gibt es noch ein Problem. Die KI, wurde gesagt, müsse
lernfähig sein. Wenn es aber zu einem Zeitpunkt t etwas gibt, dass die KI
erst zu einem späteren Zeitpunkt t' lernt, dann könnte es sein, dass eine
Entscheidung zur Zeit t anders ausfiele, als sie zur Zeit t' ausfallen
würde, sofern das später gelernte eine Auswirkung auf die Entscheidung
hätte. Ob die KI nun eine falsche Entscheidung trifft, weil sie doch nicht,
wie geglaubt, jederzeit das Gute zum Ziel hat, oder ob das geschieht, weil
ihr zum Zeitpunkt der Entscheidung noch nicht genügend Informationen
vorlagen, um bei allem Willen zum Guten das objektiv Gute tatsächlich zu
erkennen, ist für das Resultat egal. Denn das Motiv, eine Gut-Maschine zu
bauen, war ja, dass sie praktische Entscheidungen trifft. Auch das ist ein
unlösbares Problem, da es immer unendlich vieles gibt, das man nicht weiss.

Und als letztes gibt es noch ein anderes Problem, von dem ich aber nicht
behaupten würde, es sei ein objektives. Ich bin der persönlichen Auffassung,
dass die Probleme, an denen wir selbst Schuld sind, auch von uns selbst
gelöst werden müssen, dazu haben wir IMO die moralische Pflicht (andere
Probleme, etwa solche auf Grund unverschuldeter Naturkatastrophen, können
wir evtl. nicht lösen, aber das könnte wohl auch keine KI). Wenn ich damit
recht habe, wäre es moralisch verwerflich, solch eine KI zu bauen, weil wir
uns damit aus der Verantwortung stehlen würden.


Gruß,
Kurt.