Sie kann aber allein deshalb schon nicht objektiv handeln, da "gut" und
"böse" subjektive Gefühle sind, die erst dadurch, daß sie bei allen Menschen
auftauchen, einen objektiven Charakter kriegen.

[snip]


Unserem lieben Urs (ich kenn ihn auch offline) geht`s hier aber um
einen _perfekten_ Herrscher, und meiner Meinung nach ist das nicht
möglich...

Damit sich die KI nämlich auch auf alle unvorhersehbaren Situationen
einstellen kann, muss sie lernen können. Sie lernt also von Menschen. Nur
woher weiss sie, ob sie von den Guten oder den Bösen lernt, wenn dies doch
keine objektive Wahrheit ist?

Es nützt auch nichts, diese Maschine mit solchen utilitaristischen
Prinzipien wie "gr��tm�gliches Glück der gr��tm�glichen Zahl" zu
programmieren, dies würde FFI nur dazu führen, dass sie je nach Situation
Ladendiebstahl höher bestraft als schwere Körperverletzung, da man mehr
potentielle Ladendiebe abschrecken muss als Schläger...

Gruss,
Hanspeter / Philipp

--
It's 106 miles to Schardorf, we've got a full tank of gas, half a pack of
cigarettes, it's dark and we're wearing sunglasses. - Hit it.