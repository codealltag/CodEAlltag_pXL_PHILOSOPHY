Ingo Laages <syekzj@pbt.zo.tehy.cl> schrieb in im Newsbeitrag: l44x1c5dsyq.mmm@vqk.vy.xtzs.sc...
[...]

Schwierig. Wird denn die Information "über die Quelle" durch die Entropie
gemessen? Also hier etwa: Alle Zeichen eines Alphabets werden gleich
wahrscheinlich gesendet, gegenüber: Ein Zeichen mit der Wahrscheinlichkeit 1,
die andere mit der Wahrscheinlichkeit 0? (Siehe Dein Men-Beispiel.)

[...]

Wohl gerade nicht: Information für mich sehe ich als Kenntniszunahme,
die mit einer Bedeutung für mich verbunden ist. Wenn ich sie habe, kann
ich sie nicht außer acht lassen, und vorher kann ich mich nicht darauf
beziehen.

[...]

Unter meinem Wissen verstehe ich etwas in mir, das zu irgendwelchen
Gegebenheiten korrespondiert (unbeholfen ausgedrückt, nur zur
Abgrenzung). Die sicher dazugehörende Ordnung finde ich aber nicht
in dem Begriff enthalten. Bei der Gelegenheit: Vergleichbares Wissen
über einen Sachverhalt bei verschiedenen Personen kann nach meiner
Auffassung für diese Personen sehr unterschiedlich informativ sein.

Kann sein, daß wir hier unterschiedliche Fragen zu beantworten
versuchen.

Gruß, Patrick