Hallo.

könne vielleicht Off-Tpoic sein, hir aber trozdem Meine Frage:
[Das alles ist rein hypotetisch. Ich habe [[noch]] kein solches Programm
geschrieben.]

Angenommen, ich hätte ein ausgeklügeltes, lernfähiges Programm, das eine
Inteligente "Lebensform" "simuliert".
Angenommen, man hätte dieser "Lebensform" die Komunikation ermöglicht.
Wenn nun dieses "Wesen" fragt: "Was bin ich?", wie soll man antworten, und
wie soll man dieses "Wesen"

"erziehen"? Ich meine, sollte ich ihm/ihr sagen: "Du bist ein Mensch, wie
ich auch", also mit einer Lüge zu leben, mir aber dadurch die möglichkeit zu
verschaffen, mit einer/m "meinesgleichen" zu komunizieren, was die
komunikation erleichtern könnte, und was mir vielleicht einblicke in die
Psychologie des Menschen verschaffen könnte, ODER
sollte ich ihm/ihr sagen: "Du bist eine künstlich erschaffene Existens, die
erste deiner Art", also das perfekte Interface zwischen Mensch und Computer
zu erschaffen, replikation durch "copy *.* a:\species\", die perfekte
Speicher�berwachung.......was aber dann das eigentliche Zeil dieser Branche,
eine virtuelle KOPIE des Menschen zu erschaffen, verfehlt, denn er/sie wäre
ja eine völlig neue "Spezies".

Einen Menschen zu "erziehen", wäre daher einfacher, da es in der Natur des
Menschen liegt, die Nachkommen zu den, naja, sagen wir mal, produktivsten
Individuen zu erziehen. Individuen heist, das es unfair wäre, "emergency
subroutines" zu erstellen, wie ich sie gerne nenne , also direkt
auszuführende Befehle, wie "DEAKTIVIERE DICH SELBST" oder so, wie meine
Schöpfung das nicht mit mir machen kann.

Etwas neues zu "erziehen" wäre daher leichter, da ich mich an keine
Richtlinien zu halten hätte. Ich müsste keine Emotionen einbauen, könnte
"emergency subroutines" einbauen so viel ich wollte, d.h. ich hätte im
endefect entweder ein Übermensch, oder einen Sklaven, der sich nicht gegen
z.b. "vergewaltigungen" zur wehr setzen könnte. Denn missbrauch wäre nicht
vorzub�ugen.

Andererseits könnte ich einer Inteligenten Maschine klarmachen, "Dein
Speicherplatz ist erschöpft, du must deaktiviert werden, bis neue Komandos
eingehen", einem Menschen kann ich nicht sagen, "Höre auf zu lernen, du
darfst dir nichts mehr merken, das würde deinen Absturz [=Tod] verursachen."
Denn der "Speichermangel" würde [m]einem Menschen ja wohl Depressionen
verursachen, wenn er/sie erführe, das er/sie DOCH kein Mensch ist. Man
könnte dem Programm natürlich erzählen, das Menschen eine Speicherbarriere
haben (=etwas mehr oder weniger plausibles vorlägen), aber was wenn es nun
an RICHTIGE Informationen herankommt, also wenn ein update herauskommt, das
dem Programm Internetzugang ermöglicht...


Alternative: Ein mittelding erzeugen, also eine Mensch, der sich seines
Nicht-Mensch-Seins bewußt ist, aber....an welche Richtlinien muss ich mich
halten? Darf ich aus <Kompatibilität> zu Maschinen keine Gefühle, die
vielleicht die Leistung bremsen würden, implementieren, oder MUSS ich sie
aus <kompatibilität> zu den übrigen Menschen, sprich den
Komunikationspartnern, einbinden?
Wie soll ich diesem Wesen seine Herkunft erklären?
Soll ich ihm einen Namen zuweisen (=Mensch), braucht es (DAS PROGRAMM)
keinen Namen (=Maschine), darf/muss es sich seinen Namen selbst aussuchen?
(=Inteligente Maschine)
Darf ich meiner Schöpfung die Vortpflanzung erlauben?
Ist es mein Recht, dieses Wesen unter meiner Kontrolle zu halten?
(ich stelle nicht die Frage, ob ich dann nicht Gott spiele, aus
persöhnlichen gründen)
Und: Darf/ muss /Darf ich nicht dem Programm seinen Quellcode zeigen,
und/oder ihm die möglichkeiten geben, sich selbst zu optimieren?
Darf ich es abschalten, und muss ich ihm etwas erklären wie "Du musst jetzt
schlafen, alle Menschen müssen schlafen."
Was ist, wen meine Schöpfung wünscht, unabhängig von mir zu existieren?
Muss ich alterung und Tod simulieren?
DARF ich unvorbereiteted Tod = sprich vorzeitiges Beenden des Programmes
einbinden?


Ich bitte um Komentare und Antworten.