Lothar: Information kann man überhaupt nicht messen. Wenn wir die 
Wahrscheinlichkeit einer Anordnung kennen, so können wir beispielsweise 
die "Informationsarbeit" (Energie) berechnen, und zwar mit Hilfe der 
Boltzmann-Konstante. Dies wird leicht erkennbar, wenn wir Entropie und 
Information in Begriffen der Wahrscheinlichkeit ausdrücken. Hier geht es 
um die Entropieänderung, die mit der Bildung einer bestimmten Anordnung 
einer bestimmten Wahrscheinlichkeit einhergeht. Beispiel: Synthese eines 
Moleküls. Daher sprach ich ja bereits davon, dass das einzige, was in der 
Natur an sich auf Information verweist, die relativen Häufigkeiten von 
Ereignissen (Anordnungen) sind. Es könnte nichts schaden, wenn die über 
evolutionäre Informationsflüsse fabulierst, zunächst einmal versuchen 
wolltest zu verstehen, was Wahrscheinlichkeit ist. Informationen fließen 
nicht, als ob sie Wasser wären oder sonstwie irgend eine an sich seiende 
Entität.

Nick Orlando

- -

http://yddrfiygstdqjkkddkw.lhsqpovx.dh